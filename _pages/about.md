---
layout: about
title: about
permalink: /
subtitle: <a href="https://profiles.imperial.ac.uk/j.deng16/about">Assistant Professor at Imperial College London</a>

profile:
  align: right
  image: pic.jpg
  image_circular: true # crops the image to make it circular
  more_info: >

selected_papers: false # includes a list of papers marked as "selected={true}"
social: false # includes social icons at the bottom of the page

announcements:
  enabled: true # includes a list of news items
  scrollable: true # adds a vertical scroll bar if there are more than 3 news items
  limit: 5 # leave blank to include all the news in the `_news` folder
---

My research focuses on two main areas: (1) **multi-modal foundation models**, with an emphasis on perceiving, understanding, and modelling complex multi-sensory signals such as visual, acoustic, tactile, and EEG data; and (2) **generative modeling of the physical world**, with the goal of synthesizing scalable and reliable digital assets that reproduce real-world entities. My research is at the intersection of computer vision and real-world applications, striving to pioneer transformative technologies for social benefit. I actively serve as an Area Chair for leading conferences in computer vision and machine learning, including CVPR, ICCV, ECCV, NeurIPS, ICLR, ICML and AAAI. I am also an Associate Editor for IEEE Transactions on Image Processing and  Neural Networks.

Previously, I obtained my Ph.D.(2020) from the [IBUG](https://ibug.doc.ic.ac.uk/home) group under the supervision of Prof. [Stefanos Zafeiriou](https://wp.doc.ic.ac.uk/szafeiri/). My doctoral research focused on deep face analysis and modeling, encompassing efficient geometry estimation, robust feature embedding, and photorealistic texture modeling. I developed algorithms and systems ([InsightFace](https://github.com/deepinsight/insightface/)) to capture, represent, and synthesize diverse human faces with high efficiency, robustness, and fidelity. Additionally, I worked in the field of visual representation learning and won many visual perception challenges (e.g., [ImageNet](https://image-net.org/challenges/beyond_ilsvrc) and [ActivityNet](http://activity-net.org/challenges/2017/program.html)) in the past years. 

<a href="https://scholar.google.com/citations?user=Z_UoQFsAAAAJ&hl=en" target="_blank" style="margin-right: 15px"><i class="ai ai-google-scholar ai-lg"></i> Google Scholar</a>
<a href="https://github.com/jiankangdeng" target="_blank" style="margin-right: 15px"><i class="fab fa-github fa-lg"></i> Github</a>
<a href="https://twitter.com/jiankangdeng" target="_blank" style="margin-right: 15px"><i class="fab fa-twitter fa-lg"></i> Twitter</a>
<a href="https://www.linkedin.com/in/jiankangdeng" style="margin-right: 15px"><i class="fab fa-linkedin-in fa-lg"></i> LinkedIn</a>
<a href="mailto:j.deng16@imperial.ac.uk" style="margin-right: 15px"><i class="far fa-envelope-open fa-lg"></i> Mail</a>
